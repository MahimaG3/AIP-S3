{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BT3736BmmMF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbJdbJH6l6S4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbIBnxYWmD8r"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/neuml/txtai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKLIQOhjmEf3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install txtai[pipeline]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNUJ1HTFmGzK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install Pillow==9.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PnBZbsMuQFu"
      },
      "outputs": [],
      "source": [
        "# function for pdf\n",
        "\n",
        "# def my_function():\n",
        "#   textract = Textractor(sentences=True)\n",
        "\n",
        "#   data_lines = []\n",
        "#   for i in (locations_max):\n",
        "#     lines = textract(i)\n",
        "#     data_lines.append(lines)\n",
        "#   total_lines = []\n",
        "#   for i in data_lines:\n",
        "#     total_lines += i\n",
        "#   seq = embeddings.similarity(quer, total_lines)\n",
        "#   three_most = seq[0:3]\n",
        "#   indexes = []\n",
        "#   for i in three_most:\n",
        "#     indexes.append(i[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cOkgB3TmVJ8",
        "outputId": "597ab724-1b42-49f7-8e0c-b5c8aa29fa71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from txtai.pipeline import Textractor\n",
        "from txtai.embeddings import Embeddings\n",
        "# Create embeddings model, backed by sentence-transformers & transformers\n",
        "embeddings = Embeddings({\"path\": \"sentence-transformers/nli-mpnet-base-v2\"})\n",
        "\n",
        "#url = \"https://media-exp2.licdn.com/dms/image/C4E0BAQEAHtacZRTV3w/company-logo_200_200/0/1600620461329?e=2147483647&v=beta&t=7YucXpIlWxkxaQASqxqfmQ0kFTE64d8YHF9S6VQeNgE\"\n",
        "\n",
        "st.title(\"# Welcome to Ekatra.one\")\n",
        "#st.image(url, width = 150)\n",
        "\n",
        "st.markdown('_This is QA system_')\n",
        "\n",
        "a = st.sidebar.radio(\"SELECT -\", ['PDF', 'Website'])\n",
        "#data_lines = []\n",
        "#total_lines = []\n",
        "#indexes = []\n",
        "\n",
        "def my_function_pdf():\n",
        "  textract = Textractor(sentences=True)\n",
        "  data_lines = []\n",
        "  for i in (locations_max):\n",
        "    lines = textract(i)\n",
        "    if len(lines)>15:\n",
        "      data_lines.append(lines)\n",
        "  total_lines = []\n",
        "  for i in data_lines:\n",
        "    total_lines += i\n",
        "  seq = embeddings.similarity(quer, total_lines)\n",
        "  three_most = seq[0:3]\n",
        "  indexes = []\n",
        "  for i in three_most:\n",
        "    indexes.append(i[0])\n",
        "  for j in indexes:\n",
        "    st.write(total_lines[j])\n",
        "\n",
        "def my_web_scrap():\n",
        "  #Web Scraping\n",
        "  import bs4 as bs\n",
        "  import urllib.request\n",
        "  import re\n",
        "  textract = Textractor(sentences=True)\n",
        "  data_lines = []\n",
        "  total_lines = []\n",
        "  article_text = \" \"\n",
        "  for i in (locations_max):\n",
        "    #print(i)\n",
        "    scraped_data = urllib.request.urlopen(i)\n",
        "    article = scraped_data.read()\n",
        "    parsed_article = bs.BeautifulSoup(article,'lxml')\n",
        "    paragraphs = parsed_article.find_all('p')\n",
        "    for p in paragraphs:\n",
        "      article_text += p.text\n",
        "    lines = textract(i)\n",
        "    if len(lines)>15:\n",
        "      data_lines.append(lines)\n",
        "  total_lines = []\n",
        "  for i in data_lines:\n",
        "    total_lines += i\n",
        "  seq = embeddings.similarity(quer, total_lines)\n",
        "  three_most = seq[0:3]\n",
        "  indexes = []\n",
        "  for i in three_most:\n",
        "    indexes.append(i[0])\n",
        "  for j in indexes:\n",
        "    st.write(total_lines[j])\n",
        "    \n",
        "\n",
        "\n",
        "if a == 'PDF' :\n",
        "  number = st.number_input('Insert a number of files -',value =1, step =1)\n",
        "  st.write('The current number is ', number)\n",
        "  st.markdown(\"---\")\n",
        "  locations_max = []\n",
        "  for i in range (number) :\n",
        "    loc = st.text_input('Enter the PDF path :', placeholder = 'ex- /content/drive/MyDrive/', key = i)\n",
        "    locations_max.append(loc)\n",
        "\n",
        "  # for query\n",
        "  quer = st.text_input('ask me anything!', placeholder = 'what is AI?')\n",
        "  st.write('Your query is - ', quer)\n",
        "\n",
        "  # for textraction\n",
        "  if st.button('Confirm!'):\n",
        "     st.write('Confirmed')\n",
        "     my_function_pdf()\n",
        "  else:\n",
        "     st.write('ok try')\n",
        "\n",
        "if a == 'Website':\n",
        "  number = st.number_input('Insert a number of Links -',value =1, step =1)\n",
        "  st.write('The current number is ', number)  \n",
        "  st.markdown(\"---\")\n",
        "  locations_max = []\n",
        "  for i in range (number) :\n",
        "    loc = st.text_input('Enter the URL :', placeholder = 'ex- https:\\\\', key = i)\n",
        "    locations_max.append(loc)\n",
        "\n",
        "  # for query\n",
        "  quer = st.text_input('ask me anything!', placeholder = 'what is AI?')\n",
        "  st.write('Your query is - ', quer)\n",
        "\n",
        "  # for textraction\n",
        "  if st.button('Confirm!'):\n",
        "     st.write('Confirmed')\n",
        "     my_web_scrap()\n",
        "  else:\n",
        "     st.write('ok try')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "iykT-zDom7vZ",
        "outputId": "052811a2-9886-4da8-bc0d-b2c50fabd5bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-20 13:22:41.973 INFO    numexpr.utils: NumExpr defaulting to 2 threads.\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.86.176.26:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.253s\n",
            "your url is: https://long-rockets-refuse-34-86-176-26.loca.lt\n",
            "2022-06-20 13:23:24.719 Loading faiss with AVX2 support.\n",
            "2022-06-20 13:23:24.719 Could not load library with AVX2 support due to:\n",
            "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
            "2022-06-20 13:23:24.719 Loading faiss.\n",
            "2022-06-20 13:23:24.733 Successfully loaded faiss.\n",
            "Downloading: 100% 587/587 [00:00<00:00, 1.01MB/s]\n",
            "Downloading: 100% 418M/418M [00:06<00:00, 70.0MB/s]\n",
            "Downloading: 100% 1.16k/1.16k [00:00<00:00, 1.85MB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 30.2MB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 45.2MB/s]\n",
            "Downloading: 100% 239/239 [00:00<00:00, 421kB/s]\n",
            "2022-06-20 13:24:44,317 [ScriptRunner] [INFO ]  Retrieving https://www.geeksforgeeks.org/machine-learning/ to /tmp/machine-learning.\n",
            "2022-06-20 13:24:44.317 Retrieving https://www.geeksforgeeks.org/machine-learning/ to /tmp/machine-learning.\n",
            "2022-06-20 13:24:44,388 [ScriptRunner] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar to /tmp/tika-server.jar.\n",
            "2022-06-20 13:24:44.388 Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar to /tmp/tika-server.jar.\n",
            "2022-06-20 13:24:44,681 [ScriptRunner] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar.md5 to /tmp/tika-server.jar.md5.\n",
            "2022-06-20 13:24:44.681 Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar.md5 to /tmp/tika-server.jar.md5.\n",
            "2022-06-20 13:24:45,045 [ScriptRunner] [WARNI]  Failed to see startup log message; retrying...\n",
            "2022-06-20 13:24:45.045 Failed to see startup log message; retrying...\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "2022-06-20 13:25:14,315 [ScriptRunner] [INFO ]  Retrieving https://www.geeksforgeeks.org/machine-learning/ to /tmp/machine-learning.\n",
            "2022-06-20 13:25:14.315 Retrieving https://www.geeksforgeeks.org/machine-learning/ to /tmp/machine-learning.\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy_of_Web_Scraping_+_PDF_(wed15_vruksh).ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}